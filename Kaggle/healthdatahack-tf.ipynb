{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"base on: https://www.tensorflow.org/tutorials/images/segmentation","metadata":{}},{"cell_type":"markdown","source":"### Краткий обзор решений победителей:\n\n#### Bigcell\n\nMobilenet-V2, EfficientNet B2/B4 Unet\n\nMobilenet-V2 with FPN\n\nCV: training on one split\n\nPytorch lightning, pytorch-toolbelt (infrerene for wsi-images), Segmentation models pytorch, albumentations\n\nhistomicstk (stain-augmentation), Digital slide archive scikit-image: color correction, tqdm, joblib, fire, omegaconf\n\nGDAL (for WSI), openslide\n\nsettings in yaml files\n\nBCE BDice loss\n\nсмешанный взвешенный loss BCE+BDICE, pretrain on BCE+BDICE, final on BDICE, Adam opt (0.007-0.005)\n\nselfwritten H&E augmentation StainPertutbation (from histomic)\n\nColor transfer (skexp.math_histograms)\n\nTraining on crops\n\nExternal public dataset (DigestPath 2019)\n\n#### Третий глаз\n\nResnet101/ MobileNetv3 + DeepLabV3\n\nCrops 1024*1024, filter crops with empty masks\n\nNoisy data in dataset\n\nAugmentations: Distortion, ElasticTransform, OpticalTransform. CLAHE\n\nПроверка корреляции между исходным и аугментированным изображением (матрица грамма -> SVD, first k vecors -> correlation)\n\nКоррекция гистограммы\n\nepochs = 50\nbatch = 16\n1e-3\nBCE\n\nafter post training on all data:\nepochs=5\nlr 1e-5\n\n#### - \n\n* Обучение на патчах \n* pretrain on imagenet\n* IoULoss\n* EfficientNet-B7\n* OneCycleLR\n* Adam\n\n* Test Time Augmentations (rotate 90, H/V Flips)\n* Ansemble of two models: 1024*1024, 2048*2048\n\n#### -\n\nDiceLoss, AdamW\n\nAugs: Sharpen, Random flips, Random rotate, Random brightness and contrast\n\nFinal Model: ResNest50 + Unet","metadata":{}},{"cell_type":"code","source":"# !pip install git+https://github.com/qubvel/classification_models.git\n# !pip3 install tf-models-official\n# !pip install -U tensorflow-addons","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:31:57.055774Z","iopub.execute_input":"2022-04-10T12:31:57.056039Z","iopub.status.idle":"2022-04-10T12:31:57.059945Z","shell.execute_reply.started":"2022-04-10T12:31:57.056008Z","shell.execute_reply":"2022-04-10T12:31:57.059245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/tensorflow/examples.git","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:31:57.103428Z","iopub.execute_input":"2022-04-10T12:31:57.103714Z","iopub.status.idle":"2022-04-10T12:32:10.377442Z","shell.execute_reply.started":"2022-04-10T12:31:57.103685Z","shell.execute_reply":"2022-04-10T12:32:10.376388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport tensorflow.keras.backend as K\nfrom tensorflow_examples.models.pix2pix import pix2pix\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport cv2\nimport os\nimport math\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-10T12:54:18.026341Z","iopub.execute_input":"2022-04-10T12:54:18.026914Z","iopub.status.idle":"2022-04-10T12:54:18.032655Z","shell.execute_reply.started":"2022-04-10T12:54:18.026875Z","shell.execute_reply":"2022-04-10T12:54:18.031879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    SEED = 1\n#     base_path = '../input/tissuesegment-train/train/'\n    base_path = '../input/tissuesegment/tissue-segment/'\n    test_path = '../input/tissuesegment-test/test-public-images/'\n    N_CLASSES = 1\n    N_CHANNELS = 3\n    IMG_SIZE = 512\n    IMG_H = 512 # 768\n    IMG_W = 512\n#     TEST_IMG_H = 2048\n#     TEST_IMG_W = 2048\n    TEST_BATCH_SIZE = 1\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n    BATCH_SIZE = 32\n    NUMS_EXAMPLES = 220\n    STEPS_PER_EPOCH = NUMS_EXAMPLES // BATCH_SIZE # 50 - numbers of training examples (pair image - mask)\n    BUFFER_SIZE = 32\n    EPOCHS = 300\n    SEED = 42\n    \ntf.random.set_seed(Config.SEED)\nrand_generator=tf.random.Generator.from_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:10.394599Z","iopub.execute_input":"2022-04-10T12:32:10.395386Z","iopub.status.idle":"2022-04-10T12:32:10.411833Z","shell.execute_reply.started":"2022-04-10T12:32:10.395346Z","shell.execute_reply":"2022-04-10T12:32:10.411071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DataSet","metadata":{}},{"cell_type":"code","source":"len(glob(Config.base_path + \"*.jpg\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:10.414344Z","iopub.execute_input":"2022-04-10T12:32:10.414955Z","iopub.status.idle":"2022-04-10T12:32:10.427997Z","shell.execute_reply.started":"2022-04-10T12:32:10.414916Z","shell.execute_reply":"2022-04-10T12:32:10.42723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sorted(glob(Config.base_path + \"*.jpg\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:10.429105Z","iopub.execute_input":"2022-04-10T12:32:10.432283Z","iopub.status.idle":"2022-04-10T12:32:10.436453Z","shell.execute_reply.started":"2022-04-10T12:32:10.432242Z","shell.execute_reply":"2022-04-10T12:32:10.435511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len([item for item in glob(Config.base_path + \"*.jpg\") if not item.split('/')[-1].endswith('_mask.jpg')])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:10.4383Z","iopub.execute_input":"2022-04-10T12:32:10.438893Z","iopub.status.idle":"2022-04-10T12:32:10.450707Z","shell.execute_reply.started":"2022-04-10T12:32:10.438853Z","shell.execute_reply":"2022-04-10T12:32:10.449954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = [item for item in glob(Config.base_path + \"*.jpg\") if not item.split('/')[-1].endswith('_mask.jpg')]\nmasks = [item.replace('.jpg', '_mask.jpg') for item in images]\nmasks = [mask for mask in masks if os.path.isfile(mask)]\nimages = [image for image in images if os.path.isfile(image.replace('.jpg', '_mask.jpg'))]\nimage_mask_pairs = list(zip(images, masks))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:10.452291Z","iopub.execute_input":"2022-04-10T12:32:10.452818Z","iopub.status.idle":"2022-04-10T12:32:10.56911Z","shell.execute_reply.started":"2022-04-10T12:32:10.452779Z","shell.execute_reply":"2022-04-10T12:32:10.568408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(image_mask_pairs)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:10.570528Z","iopub.execute_input":"2022-04-10T12:32:10.570919Z","iopub.status.idle":"2022-04-10T12:32:10.57734Z","shell.execute_reply.started":"2022-04-10T12:32:10.57085Z","shell.execute_reply":"2022-04-10T12:32:10.576359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_crop(image, mask, shape=(128, 128, 4)): # concat mask and image\n#     stacked_image = tf.stack([image, mask], axis=0)\n#     cropped_image = tf.image.random_crop(\n#         stacked_image, size=[2, *shape])\n\n#     return cropped_image[0], cropped_image[1]\n    concat=tf.concat([image, mask], axis=2)\n    crop=tf.image.random_crop(concat, size=shape)\n    return crop[..., :3], tf.expand_dims(crop[..., 3], -1)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:10.579512Z","iopub.execute_input":"2022-04-10T12:32:10.580517Z","iopub.status.idle":"2022-04-10T12:32:10.588324Z","shell.execute_reply.started":"2022-04-10T12:32:10.580473Z","shell.execute_reply":"2022-04-10T12:32:10.587281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_image(img_mask_path: str) -> dict:\n    \"\"\"Load an image and its annotation (mask) and returning\n    a dictionary.\n\n    Parameters\n    ----------\n    img_path : str\n        Image (not the mask) location.\n\n    Returns\n    -------\n    dict\n        Dictionary mapping an image and its annotation.\n    \"\"\"\n    tf.print(img_mask_path) # DEBUG\n    image = tf.io.read_file(img_mask_path[0])\n    image = tf.io.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.uint8)\n\n    mask = tf.io.read_file(img_mask_path[1])\n    mask = tf.io.decode_jpeg(mask, channels=1)\n    \n    mask = tf.where(mask < 128, np.dtype('uint8').type(0), mask)\n    mask = tf.where(mask >= 128, np.dtype('uint8').type(1), mask)\n    \n    return {'image': image, 'segmentation_mask': mask}\n\ndef parse_image_test(img_path: str) -> dict:\n    \"\"\"Load an image and its annotation (mask) and returning\n    a dictionary.\n\n    Parameters\n    ----------\n    img_path : str\n        Image (not the mask) location.\n\n    Returns\n    -------\n    dict\n        Dictionary mapping an image and its annotation.\n    \"\"\"\n    image = tf.io.read_file(img_path)\n    image = tf.io.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.uint8)\n\n    return {'image': image, 'image_path': img_path}\n\n@tf.function\ndef normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n    \"\"\"Rescale the pixel values of the images between 0.0 and 1.0\n    compared to [0,255] originally.\n\n    Parameters\n    ----------\n    input_image : tf.Tensor\n        Tensorflow tensor containing an image of size [SIZE,SIZE,3].\n    input_mask : tf.Tensor\n        Tensorflow tensor containing an annotation of size [SIZE,SIZE,1].\n\n    Returns\n    -------\n    tuple\n        Normalized image and its annotation.\n    \"\"\"\n    input_image = tf.cast(input_image, tf.float32) / 255.0\n#     input_mask -= 1 ONLY for labels starts with zero !\n#     input_mask = tf.cast(input_mask, tf.float32) / 255.0\n    return input_image, input_mask\n\n@tf.function\ndef normalize_test(input_image: tf.Tensor):\n    input_image = tf.cast(input_image, tf.float32) / 255.0\n    return input_image\n\n@tf.function\ndef load_image_train(datapoint: dict) -> tuple:\n    \"\"\"Apply some transformations to an input dictionary\n    containing a train image and its annotation.\n\n    Notes\n    -----\n    An annotation is a regular  channel image.\n    If a transformation such as rotation is applied to the image,\n    the same transformation has to be applied on the annotation also.\n\n    Parameters\n    ----------\n    datapoint : dict\n        A dict containing an image and its annotation.\n\n    Returns\n    -------\n    tuple\n        A modified image and its annotation.\n    \"\"\"\n    # tf.image.crop_to_bounding_box or tf.image.crop_and_resize\n    input_image = datapoint['image']\n    input_mask = datapoint['segmentation_mask']\n    \n    # Central crop\n#     input_image = tf.image.central_crop(input_image, central_fraction=0.5)\n#     input_mask = tf.image.central_crop(input_mask, central_fraction=0.5)\n\n    # Random crop\n    CropToH, CropToW = 2048, 2048\n    if (tf.shape(input_image)[0] >= CropToH) and (tf.shape(input_image)[1] >= CropToW):\n#         input_image, input_mask = random_crop(image=input_image, mask=input_mask, shape=(CropToH, CropToW, 3))\n        seed=rand_generator.uniform_full_int([2],dtype=tf.int32)\n        input_image = tf.image.stateless_random_crop(input_image, (CropToH,CropToW,3), seed)\n        input_mask = tf.image.stateless_random_crop(input_mask, (CropToH,CropToW,1), seed)\n    \n    input_image = tf.image.resize(input_image, (Config.IMG_H, Config.IMG_W))\n    input_mask = tf.image.resize(input_mask, (Config.IMG_H, Config.IMG_W))\n    \n    if input_image.shape[0] > input_image.shape[1]:\n        input_image = tf.image.rot90(input_image)\n        input_mask = tf.image.rot90(input_mask)\n\n    if tf.random.uniform(()) > 0.5:\n        input_image = tf.image.flip_left_right(input_image)\n        input_mask = tf.image.flip_left_right(input_mask)\n\n    input_image, input_mask = normalize(input_image, input_mask)\n\n    return input_image, input_mask\n\n@tf.function\ndef load_image_test(datapoint: dict) -> tuple:\n    \"\"\"Normalize and resize a test image and its annotation.\n\n    Notes\n    -----\n    Since this is for the test set, we don't need to apply\n    any data augmentation technique.\n\n    Parameters\n    ----------\n    datapoint : dict\n        A dict containing an image and its annotation.\n\n    Returns\n    -------\n    tuple\n        A modified image and its annotation.\n    \"\"\"\n#     input_image = tf.image.resize(datapoint['image'], (Config.IMG_H, Config.IMG_W))\n#     input_image = tf.image.resize(datapoint['image'], (Config.TEST_IMG_H, Config.TEST_IMG_W))\n    \n    input_image = datapoint[\"image\"]\n    \n    if (tf.shape(input_image)[0] != None):\n        input_image_h = tf.shape(input_image)[0]\n        input_image_w = tf.shape(input_image)[1]\n#         target_h = (input_image_h // 64)*64 # 32\n#         target_w = (input_image_w // 64)*64 # 32\n        target_h = tf.cast(tf.math.ceil(tf.cast(input_image_h, dtype=tf.float32) / 64.), dtype=tf.int32)*64\n        target_w = tf.cast(tf.math.ceil(tf.cast(input_image_w, dtype=tf.float32) / 64.), dtype=tf.int32)*64\n        input_image = tf.image.resize_with_crop_or_pad(input_image, target_h, target_w)\n#         input_image = tf.image.resize(input_image, (target_h ,target_w), tf.image.ResizeMethod.LANCZOS5)\n    \n#     if input_image.shape[0] > input_image.shape[1]:\n#         input_image = tf.image.rot90(input_image)\n\n    input_image = normalize_test(input_image)\n\n    return input_image, datapoint[\"image_path\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:39:52.225501Z","iopub.execute_input":"2022-04-10T12:39:52.225817Z","iopub.status.idle":"2022-04-10T12:39:52.252712Z","shell.execute_reply.started":"2022-04-10T12:39:52.225783Z","shell.execute_reply":"2022-04-10T12:39:52.251979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices(image_mask_pairs)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:39:57.963908Z","iopub.execute_input":"2022-04-10T12:39:57.964204Z","iopub.status.idle":"2022-04-10T12:39:57.971418Z","shell.execute_reply.started":"2022-04-10T12:39:57.964172Z","shell.execute_reply":"2022-04-10T12:39:57.970366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(parse_image)\ntrain_dataset = train_dataset.map(load_image_train, num_parallel_calls=Config.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:39:59.025064Z","iopub.execute_input":"2022-04-10T12:39:59.025668Z","iopub.status.idle":"2022-04-10T12:39:59.287323Z","shell.execute_reply.started":"2022-04-10T12:39:59.025633Z","shell.execute_reply":"2022-04-10T12:39:59.286535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = tf.data.Dataset.list_files(Config.test_path+'*.jpg', shuffle=False)\ntest_dataset = test_dataset.map(parse_image_test)\ntest_dataset = test_dataset.map(load_image_test)\ntest_dataset = test_dataset.batch(Config.TEST_BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:11.01318Z","iopub.execute_input":"2022-04-10T12:32:11.013379Z","iopub.status.idle":"2022-04-10T12:32:11.360124Z","shell.execute_reply.started":"2022-04-10T12:32:11.013354Z","shell.execute_reply":"2022-04-10T12:32:11.359415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Augmentation","metadata":{}},{"cell_type":"code","source":"class Augment(tf.keras.layers.Layer):\n  def __init__(self, seed=42):\n    super().__init__()\n    # both use the same seed, so they'll make the same random changes.\n    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=Config.SEED)\n    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=Config.SEED)\n\n  def call(self, inputs, labels):\n    inputs = self.augment_inputs(inputs)\n    labels = self.augment_labels(labels)\n    return inputs, labels","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:11.361573Z","iopub.execute_input":"2022-04-10T12:32:11.361821Z","iopub.status.idle":"2022-04-10T12:32:11.370929Z","shell.execute_reply.started":"2022-04-10T12:32:11.361787Z","shell.execute_reply":"2022-04-10T12:32:11.369965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From: https://www.kaggle.com/code/cdeotte/rotation-augmentation-gpu-tpu-0-96/notebook\n\nWork only for square image shape","metadata":{}},{"cell_type":"code","source":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:36:33.433855Z","iopub.execute_input":"2022-04-10T12:36:33.434143Z","iopub.status.idle":"2022-04-10T12:36:33.445254Z","shell.execute_reply.started":"2022-04-10T12:36:33.434107Z","shell.execute_reply":"2022-04-10T12:36:33.443469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform(image, label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = Config.IMG_H\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 15. * tf.random.normal([1],dtype='float32', seed=Config.SEED)\n    shr = 5. * tf.random.normal([1],dtype='float32', seed=Config.SEED)\n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32', seed=Config.SEED)/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32', seed=Config.SEED)/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32', seed=Config.SEED) \n    w_shift = 16. * tf.random.normal([1],dtype='float32', seed=Config.SEED) \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(Config.IMG_H//2,-Config.IMG_W//2,-1), DIM )\n    y = tf.tile( tf.range(-Config.IMG_H//2,Config.IMG_W//2),[DIM] )\n    z = tf.ones([Config.IMG_H*Config.IMG_W],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-Config.IMG_H//2+XDIM+1,Config.IMG_W//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [Config.IMG_H//2-idx2[0,], Config.IMG_W//2-1+idx2[1,]] )\n    d = tf.gather_nd(image, tf.transpose(idx3))\n    \n    label = tf.gather_nd(label,tf.transpose(idx3))\n    label = tf.reshape(label,[Config.IMG_H,Config.IMG_W,1])\n        \n    return tf.reshape(d,[Config.IMG_H,Config.IMG_W,3]), label","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:36:35.397851Z","iopub.execute_input":"2022-04-10T12:36:35.398133Z","iopub.status.idle":"2022-04-10T12:36:35.410466Z","shell.execute_reply.started":"2022-04-10T12:36:35.398096Z","shell.execute_reply":"2022-04-10T12:36:35.409521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(transform)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:40:04.771568Z","iopub.execute_input":"2022-04-10T12:40:04.772145Z","iopub.status.idle":"2022-04-10T12:40:04.862678Z","shell.execute_reply.started":"2022-04-10T12:40:04.7721Z","shell.execute_reply":"2022-04-10T12:40:04.86198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Prepare DataSet for training","metadata":{}},{"cell_type":"code","source":"# train_dataset = train_dataset.cache()\ntrain_dataset = train_dataset.shuffle(buffer_size=Config.BUFFER_SIZE, seed=Config.SEED)\ntrain_dataset = train_dataset.repeat()\ntrain_dataset = train_dataset.batch(Config.BATCH_SIZE)\n# train_dataset = train_dataset.map(Augment())\ntrain_dataset = train_dataset.prefetch(buffer_size=Config.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:40:05.854985Z","iopub.execute_input":"2022-04-10T12:40:05.85577Z","iopub.status.idle":"2022-04-10T12:40:05.865897Z","shell.execute_reply.started":"2022-04-10T12:40:05.855729Z","shell.execute_reply":"2022-04-10T12:40:05.865133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"...","metadata":{}},{"cell_type":"code","source":"def display_sample(display_list):\n    \"\"\"Show side-by-side an input image,\n    the ground truth and the prediction.\n    \"\"\"\n    plt.figure(figsize=(18, 9))\n\n    title = ['Input Image', 'True Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(2, len(display_list) // 2, i+1)\n        plt.title(title[i % 2])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()\nfor image, mask in train_dataset.take(1):\n    sample_image, sample_mask = image, mask\n\ndisplay_sample([sample_image[0], sample_mask[0], sample_image[1], sample_mask[1], sample_image[2], sample_mask[2], sample_image[3], sample_mask[3]])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:44:15.977166Z","iopub.execute_input":"2022-04-10T12:44:15.977989Z","iopub.status.idle":"2022-04-10T12:44:17.583321Z","shell.execute_reply.started":"2022-04-10T12:44:15.977946Z","shell.execute_reply":"2022-04-10T12:44:17.582109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"# from classification_models.tfkeras import Classifiers\n# SeResNeXT, preprocess_input = Classifiers.get('seresnext50')\n# base_model = SeResNeXT(include_top = False, input_shape=(None, None, 3), weights='imagenet')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.729107Z","iopub.status.idle":"2022-04-10T12:32:13.732205Z","shell.execute_reply.started":"2022-04-10T12:32:13.731926Z","shell.execute_reply":"2022-04-10T12:32:13.731959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git clone https://github.com/rishigami/Swin-Transformer-TF.git\n# import sys\n# sys.path.append('./Swin-Transformer-TF')\n# from swintransformer import SwinTransformer\n# model = SwinTransformer('swin_tiny_224', num_classes=1000, include_top=False, pretrained=True)\n\n# model = tf.keras.Sequential([\n#   tf.keras.layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data, tf.float32), mode=\"torch\"), input_shape=[*(224,224), 3]),\n#   SwinTransformer('swin_tiny_224', include_top=False, pretrained=True),\n#   tf.keras.layers.Dense(5, activation='softmax')\n# ])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.7361Z","iopub.status.idle":"2022-04-10T12:32:13.736795Z","shell.execute_reply.started":"2022-04-10T12:32:13.736538Z","shell.execute_reply":"2022-04-10T12:32:13.736564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data, tf.float32), mode=\"torch\"), input_shape=[*IMAGE_SIZE, 3])\n# pretrained_model = SwinTransformer('swin_large_224', num_classes=len(CLASSES), include_top=False, pretrained=True, use_tpu=True)\n\n# model = tf.keras.Sequential([\n#     img_adjust_layer,\n#     pretrained_model,\n#     tf.keras.layers.Dense(len(CLASSES), activation='softmax')\n# ])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.743041Z","iopub.status.idle":"2022-04-10T12:32:13.743757Z","shell.execute_reply.started":"2022-04-10T12:32:13.743496Z","shell.execute_reply":"2022-04-10T12:32:13.743527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = tf.keras.applications.MobileNetV2(input_shape=[None, None, 3], include_top=False) # 1024, 1024\n\n# Use the activations of these layers\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\nbase_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n\n# Create the feature extraction model\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n\ndown_stack.trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.750045Z","iopub.status.idle":"2022-04-10T12:32:13.750741Z","shell.execute_reply.started":"2022-04-10T12:32:13.750481Z","shell.execute_reply":"2022-04-10T12:32:13.750507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"up_stack = [\n    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.752058Z","iopub.status.idle":"2022-04-10T12:32:13.752823Z","shell.execute_reply.started":"2022-04-10T12:32:13.752544Z","shell.execute_reply":"2022-04-10T12:32:13.752571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unet_model(output_channels:int):\n  inputs = tf.keras.layers.Input(shape=[None, None, 3]) # 1024, 1024\n\n  # Downsampling through the model\n  skips = down_stack(inputs)\n  x = skips[-1]\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    concat = tf.keras.layers.Concatenate()\n    x = concat([x, skip])\n\n  # This is the last layer of the model\n  last = tf.keras.layers.Conv2DTranspose(\n      filters=output_channels, kernel_size=3, strides=2,\n      padding='same', activation='sigmoid')  #64x64 -> 128x128 # \n\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.754227Z","iopub.status.idle":"2022-04-10T12:32:13.754933Z","shell.execute_reply.started":"2022-04-10T12:32:13.754687Z","shell.execute_reply":"2022-04-10T12:32:13.754714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# doesn't work now\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.dot(y_true, K.transpose(y_pred))\n    union = K.dot(y_true,K.transpose(y_true))+K.dot(y_pred,K.transpose(y_pred))\n    return (2. * intersection + smooth) / (union + smooth)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.756284Z","iopub.status.idle":"2022-04-10T12:32:13.763445Z","shell.execute_reply.started":"2022-04-10T12:32:13.763133Z","shell.execute_reply":"2022-04-10T12:32:13.763166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def DiceLoss(targets, inputs, smooth=1e-6):\n\n   inputs = K.flatten(inputs)\n   targets = K.flatten(targets)\n\n   intersection = K.sum(targets*inputs)\n   dice = (2.*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n   return 1 - dice","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.764927Z","iopub.status.idle":"2022-04-10T12:32:13.765588Z","shell.execute_reply.started":"2022-04-10T12:32:13.765343Z","shell.execute_reply":"2022-04-10T12:32:13.765368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard_distance_loss(y_true, y_pred, smooth=100):\n    \"\"\"\n    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n    \n    The jaccard distance loss is usefull for unbalanced datasets. This has been\n    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n    gradient.\n    \n    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n    \n    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n    @author: wassname\n    \"\"\"\n    intersection = K.sum(K.sum(K.abs(y_true * y_pred), axis=-1))\n    sum_ = K.sum(K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1))\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    return (1 - jac) * smooth\n\ndef dice_metric(y_pred, y_true):\n    intersection = K.sum(K.sum(K.abs(y_true * y_pred), axis=-1))\n    union = K.sum(K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1))\n    # if y_pred.sum() == 0 and y_pred.sum() == 0:\n    #     return 1.0\n\n    return 2*intersection / union","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.766868Z","iopub.status.idle":"2022-04-10T12:32:13.767529Z","shell.execute_reply.started":"2022-04-10T12:32:13.767284Z","shell.execute_reply":"2022-04-10T12:32:13.76731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection / union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        def treat_image(log_lab):\n            log, lab = log_lab\n            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n            log, lab = flatten_binary_scores(log, lab, ignore)\n            return lovasz_hinge_flat(log, lab)\n        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n\n    def compute_loss():\n        labelsf = tf.cast(labels, logits.dtype)\n        signs = 2. * labelsf - 1.\n        errors = 1. - logits * tf.stop_gradient(signs)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n        gt_sorted = tf.gather(labelsf, perm)\n        grad = lovasz_grad(gt_sorted)\n        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n        return loss\n\n    # deal with the void prediction case (only void pixels)\n    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n                   lambda: tf.reduce_sum(logits) * 0.,\n                   compute_loss,\n                   name=\"loss\"\n                   )\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = tf.reshape(scores, (-1,))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return scores, labels\n    valid = tf.not_equal(labels, ignore)\n    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n    return vscores, vlabels","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.768846Z","iopub.status.idle":"2022-04-10T12:32:13.76955Z","shell.execute_reply.started":"2022-04-10T12:32:13.769299Z","shell.execute_reply":"2022-04-10T12:32:13.769326Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_CLASSES = 1\n\nloss =tf.keras.losses.BinaryCrossentropy(from_logits=True)\nopt_rec_adam = tfa.optimizers.RectifiedAdam(learning_rate=1e-3)\n\nsch_cos_dec = tf.keras.optimizers.schedules.CosineDecay(3e-4, 1000) # use as lr parameter\nlr_decayed_fn = (tf.keras.optimizers.schedules.CosineDecayRestarts(1e-3, 1000))  # use as lr parameter\n\n# if OUTPUT_CLASSES > 1:\n#     loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n\nmetrics = ['accuracy', dice_metric]\n\nmodel = unet_model(output_channels=OUTPUT_CLASSES)\n\nmodel.compile(optimizer='adam',\n              loss=DiceLoss, # \n              metrics=metrics)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.770911Z","iopub.status.idle":"2022-04-10T12:32:13.771589Z","shell.execute_reply.started":"2022-04-10T12:32:13.771339Z","shell.execute_reply":"2022-04-10T12:32:13.771367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.779609Z","iopub.status.idle":"2022-04-10T12:32:13.780276Z","shell.execute_reply.started":"2022-04-10T12:32:13.780014Z","shell.execute_reply":"2022-04-10T12:32:13.780043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display(display_list):\n  plt.figure(figsize=(15, 15))\n\n  title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n  for i in range(len(display_list)):\n    plt.subplot(1, len(display_list), i+1)\n    plt.title(title[i])\n    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n    plt.axis('off')\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.781525Z","iopub.status.idle":"2022-04-10T12:32:13.78221Z","shell.execute_reply.started":"2022-04-10T12:32:13.781944Z","shell.execute_reply":"2022-04-10T12:32:13.781971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_mask(pred_mask):\n#   pred_mask = tf.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0].squeeze(axis=-1)\n\ndef show_predictions(dataset=train_dataset, num=1):\n#   if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n#   else:\n#     display([sample_image, sample_mask,\n#              create_mask(model.predict(sample_image[tf.newaxis, ...]))])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.78352Z","iopub.status.idle":"2022-04-10T12:32:13.784223Z","shell.execute_reply.started":"2022-04-10T12:32:13.783948Z","shell.execute_reply":"2022-04-10T12:32:13.783976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_predictions()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.7855Z","iopub.status.idle":"2022-04-10T12:32:13.786255Z","shell.execute_reply.started":"2022-04-10T12:32:13.785987Z","shell.execute_reply":"2022-04-10T12:32:13.786015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fit","metadata":{}},{"cell_type":"code","source":"class DisplayCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    if (epoch % 50)==0:\n        clear_output(wait=True)\n        show_predictions()\n        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n    \ntensorboard_callback = tf.keras.callbacks.TensorBoard('/logdir', histogram_freq=1)\nsave_model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True)\nearly_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=10, verbose=1)\neas = EarlyStopping(monitor='val_loss', patience=8, min_delta=1e-5, verbose=1, mode='min', baseline=None, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.787571Z","iopub.status.idle":"2022-04-10T12:32:13.788272Z","shell.execute_reply.started":"2022-04-10T12:32:13.787998Z","shell.execute_reply":"2022-04-10T12:32:13.788026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import math\n# from keras.callbacks import Callback\n# from keras import backend as K\n\n\n# class CosineAnnealingScheduler(Callback):\n#     \"\"\"Cosine annealing scheduler.\n#     \"\"\"\n\n#     def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n#         super(CosineAnnealingScheduler, self).__init__()\n#         self.T_max = T_max\n#         self.eta_max = eta_max\n#         self.eta_min = eta_min\n#         self.verbose = verbose\n\n#     def on_epoch_begin(self, epoch, logs=None):\n#         if not hasattr(self.model.optimizer, 'lr'):\n#             raise ValueError('Optimizer must have a \"lr\" attribute.')\n#         lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n#         K.set_value(self.model.optimizer.lr, lr)\n#         if self.verbose > 0:\n#             print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n#                   'rate to %s.' % (epoch + 1, lr))\n\n#     def on_epoch_end(self, epoch, logs=None):\n#         logs = logs or {}\n#         logs['lr'] = K.get_value(self.model.optimizer.lr)\n\n\n\n# callbacks = [\n#     CosineAnnealingScheduler(T_max=100, eta_max=1e-2, eta_min=1e-4)\n# ]\n\n\n# class SGDRScheduler(tf.keras.callbacks.Callback):\n#     '''Cosine annealing learning rate scheduler with periodic restarts.\n#     # Usage\n#         ```python\n#             schedule = SGDRScheduler(min_lr=1e-5,\n#                                      max_lr=1e-2,\n#                                      steps_per_epoch=np.ceil(epoch_size/batch_size),\n#                                      lr_decay=0.9,\n#                                      cycle_length=5,\n#                                      mult_factor=1.5)\n#             model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n#         ```\n#     # Arguments\n#         min_lr: The lower bound of the learning rate range for the experiment.\n#         max_lr: The upper bound of the learning rate range for the experiment.\n#         steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n#         lr_decay: Reduce the max_lr after the completion of each cycle.\n#                   Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n#         cycle_length: Initial number of epochs in a cycle.\n#         mult_factor: Scale epochs_to_restart after each full cycle completion.\n#     # References\n#         Blog post: jeremyjordan.me/nn-learning-rate\n#         Original paper: http://arxiv.org/abs/1608.03983\n#     '''\n#     def __init__(self,\n#                  min_lr,\n#                  max_lr,\n#                  steps_per_epoch,\n#                  lr_decay=1,\n#                  cycle_length=10,\n#                  mult_factor=2):\n\n#         self.min_lr = min_lr\n#         self.max_lr = max_lr\n#         self.lr_decay = lr_decay\n\n#         self.batch_since_restart = 0\n#         self.next_restart = cycle_length\n\n#         self.steps_per_epoch = steps_per_epoch\n\n#         self.cycle_length = cycle_length\n#         self.mult_factor = mult_factor\n\n#         self.history = {}\n\n#     def clr(self):\n#         '''Calculate the learning rate.'''\n#         fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n#         lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n#         return lr\n\n#     def on_train_begin(self, logs={}):\n#         '''Initialize the learning rate to the minimum value at the start of training.'''\n#         logs = logs or {}\n#         K.set_value(self.model.optimizer.lr, self.max_lr)\n\n#     def on_batch_end(self, batch, logs={}):\n#         '''Record previous batch statistics and update the learning rate.'''\n#         logs = logs or {}\n#         self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n#         for k, v in logs.items():\n#             self.history.setdefault(k, []).append(v)\n\n#         self.batch_since_restart += 1\n#         K.set_value(self.model.optimizer.lr, self.clr())\n\n#     def on_epoch_end(self, epoch, logs={}):\n#         '''Check for end of current cycle, apply restarts when necessary.'''\n#         if epoch + 1 == self.next_restart:\n#             self.batch_since_restart = 0\n#             self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n#             self.next_restart += self.cycle_length\n#             self.max_lr *= self.lr_decay\n#             self.best_weights = self.model.get_weights()\n\n#     def on_train_end(self, logs={}):\n#         '''Set weights to the values from the end of the most recent cycle for best performance.'''\n#         self.model.set_weights(self.best_weights)\n\n\n\n# lr_sched = SGDRScheduler(min_lr=1e-5,\n#                          max_lr=1e-2,\n#                          steps_per_epoch=np.ceil(Config.NUMS_EXAMPLES/Config.BATCH_SIZE), # epoch_size = len(train)\n#                          lr_decay=0.85,\n#                          mult_factor=1.5)\n\n\n# class ValLossDisplay(tf.keras.callbacks.Callback):\n#     def __init__(self, loss_index='val_loss'):\n#         self.best_loss = None\n#         self.loss_index = loss_index\n#         super().__init__()\n    \n#     def on_epoch_end(self, epoch, logs=None):\n#         if self.best_loss is None or self.best_loss > logs[self.loss_index]:\n#             self.best_loss = logs[self.loss_index]\n#         print(\"\\r\",'Epoch: %d - Loss: %.6f (best: %.6f)' % (epoch, logs[self.loss_index], self.best_loss), ' '*5, end='')\n    \n#     def on_train_end(self, logs=None):\n#         print(\"\\r\",'Loss: %.6f' % (self.best_loss), ' '*40)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.795988Z","iopub.status.idle":"2022-04-10T12:32:13.796677Z","shell.execute_reply.started":"2022-04-10T12:32:13.796416Z","shell.execute_reply":"2022-04-10T12:32:13.796443Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('../input/tf-unet-model-100-epochs-768-1024-weights/tf_unet_model_100_epochs_768_1024_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.798045Z","iopub.status.idle":"2022-04-10T12:32:13.798738Z","shell.execute_reply.started":"2022-04-10T12:32:13.798476Z","shell.execute_reply":"2022-04-10T12:32:13.798503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VAL_SUBSPLITS = 5\n# VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n\nmodel_history = model.fit(train_dataset, epochs=Config.EPOCHS,\n                          steps_per_epoch=Config.STEPS_PER_EPOCH,\n#                           validation_steps=VALIDATION_STEPS,\n#                           validation_data=test_batches,\n                          callbacks=[DisplayCallback()])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.80004Z","iopub.status.idle":"2022-04-10T12:32:13.800773Z","shell.execute_reply.started":"2022-04-10T12:32:13.8005Z","shell.execute_reply":"2022-04-10T12:32:13.800526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save('tf_unet_model_100_epochs_768_1024')\nmodel.save_weights('tf_unet_model_300_epoch_512_fullds_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.802071Z","iopub.status.idle":"2022-04-10T12:32:13.802798Z","shell.execute_reply.started":"2022-04-10T12:32:13.802501Z","shell.execute_reply":"2022-04-10T12:32:13.802528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image, path in test_dataset.take(1):\n    plt.imshow(image[0])\n    title = path[0].numpy().decode('utf-8').split('/')[-1]\n    plt.title(title)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.804051Z","iopub.status.idle":"2022-04-10T12:32:13.805762Z","shell.execute_reply.started":"2022-04-10T12:32:13.805504Z","shell.execute_reply":"2022-04-10T12:32:13.805533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image, path in test_dataset:\n    plt.imshow(image[0])\n    print(image[0].shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.80742Z","iopub.status.idle":"2022-04-10T12:32:13.814404Z","shell.execute_reply.started":"2022-04-10T12:32:13.814132Z","shell.execute_reply":"2022-04-10T12:32:13.814163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict","metadata":{}},{"cell_type":"code","source":"# preds = model.predict(test_dataset)\npreds = []\nfor image, path in test_dataset:\n    h, w = image.shape[1] //2, image.shape[2] //2\n    split_1 = image[: ,:h ,: , :]\n    split_2 = image[: ,h: ,: , :]\n    \n    pred_mask_split_1 = model.predict_on_batch(split_1)\n    pred_mask_split_2 = model.predict_on_batch(split_2)\n    \n    pred_mask_full = tf.concat([pred_mask_split_1, pred_mask_split_2], axis=1)\n    print(image.shape, split_1.shape, pred_mask_full.shape)\n    preds.append(pred_mask_full)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.815816Z","iopub.status.idle":"2022-04-10T12:32:13.816541Z","shell.execute_reply.started":"2022-04-10T12:32:13.816291Z","shell.execute_reply":"2022-04-10T12:32:13.81632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# h, w = image.shape[1] //2, image.shape[2] //2 # TensorShape([1, 6080, 6912, 3])\n# split_1 = image[: ,: ,:w , :] # TensorShape([1, 6080, 3456, 3])\n# split_2 = image[: ,: ,w: , :] # TensorShape([1, 6080, 3456, 3])\n\n# tf.concat([split_1, split_2], axis=2).shape # TensorShape([1, 6080, 6912, 3])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.817788Z","iopub.status.idle":"2022-04-10T12:32:13.818429Z","shell.execute_reply.started":"2022-04-10T12:32:13.818192Z","shell.execute_reply":"2022-04-10T12:32:13.818218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_ = tf.squeeze(preds[0], axis=0)\npred_ = tf.image.convert_image_dtype(pred_, dtype=tf.int8)\npred_ = pred_.numpy()\n# pred_[pred_ < 128] = 0\n# pred_[pred_ >= 128] = 255\n# cv2.imwrite('pred.jpg',pred_)\nplt.imshow(pred_)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.81969Z","iopub.status.idle":"2022-04-10T12:32:13.820352Z","shell.execute_reply.started":"2022-04-10T12:32:13.820112Z","shell.execute_reply":"2022-04-10T12:32:13.820139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get test filenames\nfile_names = []\nfor image, path in test_dataset:\n    file_names.extend(list(path.numpy()))\nfile_names = [file.decode('utf-8').split('/')[-1] for file in file_names]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.821578Z","iopub.status.idle":"2022-04-10T12:32:13.822236Z","shell.execute_reply.started":"2022-04-10T12:32:13.821987Z","shell.execute_reply":"2022-04-10T12:32:13.822013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read original files size\ntest_img_sizes = {}\ntest_files = [item for item in glob('../input/tissuesegment-test/test-public-images/*.jpg')]\nfor file in test_files:\n    img = cv2.imread(file)\n    test_img_sizes[file.split('/')[-1]] = (img.shape[0], img.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.823542Z","iopub.status.idle":"2022-04-10T12:32:13.830397Z","shell.execute_reply.started":"2022-04-10T12:32:13.83013Z","shell.execute_reply":"2022-04-10T12:32:13.830161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Restore original size\ntry:\n    os.mkdir('predicted_masks')\nexcept FileExistsError:\n    print('dir already exist')\n\nfor i in range(0, len(preds)):\n    pred_mask = preds[i]\n    pred_mask = tf.image.convert_image_dtype(pred_mask, dtype=tf.int8)\n    \n    target_h = test_img_sizes[file_names[i]][0]\n    target_w = test_img_sizes[file_names[i]][1]\n    \n#     pred_mask = tf.image.resize(pred_mask, (target_h, target_w), method=tf.image.ResizeMethod.BICUBIC)\n    pred_mask = tf.image.resize_with_crop_or_pad(pred_mask, target_h, target_w)\n    pred_mask = tf.squeeze(pred_mask ,axis=0)\n    pred_mask = pred_mask.numpy()\n    pred_mask = pred_mask.astype(\"uint8\")\n    \n#     if target_h > target_w:\n#         pred_mask = cv2.rotate(pred_mask, cv2.cv2.ROTATE_90_CLOCKWISE)\n    \n    \n#     pred_mask = cv2.resize(pred_mask, (target_w, target_h), interpolation=cv2.INTER_LINEAR_EXACT)\n    thresh, pred_mask = cv2.threshold(pred_mask, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    cv2.imwrite(f'./predicted_masks/{file_names[i]}', pred_mask)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.831681Z","iopub.status.idle":"2022-04-10T12:32:13.832323Z","shell.execute_reply.started":"2022-04-10T12:32:13.832075Z","shell.execute_reply":"2022-04-10T12:32:13.832116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r predicted_masks.zip ./predicted_masks/*","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.833533Z","iopub.status.idle":"2022-04-10T12:32:13.834198Z","shell.execute_reply.started":"2022-04-10T12:32:13.833946Z","shell.execute_reply":"2022-04-10T12:32:13.833972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with tf.device(\"gpu:0\"):\n#     pass","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.835494Z","iopub.status.idle":"2022-04-10T12:32:13.836193Z","shell.execute_reply.started":"2022-04-10T12:32:13.835922Z","shell.execute_reply":"2022-04-10T12:32:13.835949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"try post processing","metadata":{}},{"cell_type":"code","source":"!mkdir predicted_masks_post","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.837608Z","iopub.status.idle":"2022-04-10T12:32:13.838317Z","shell.execute_reply.started":"2022-04-10T12:32:13.838054Z","shell.execute_reply":"2022-04-10T12:32:13.838081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(18, 18))\nfor i in range(15):\n    img = cv2.imread(f'./predicted_masks/{i+1}.jpg', cv2.COLOR_RGB2GRAY)\n    kernel = np.ones((5,5),np.uint8)\n    closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n# plt.imshow(closing)\n    closing = cv2.cvtColor(closing, cv2.COLOR_RGB2GRAY)\n    cv2.imwrite(f'predicted_masks_post/{i+1}.jpg', closing)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.845865Z","iopub.status.idle":"2022-04-10T12:32:13.846519Z","shell.execute_reply.started":"2022-04-10T12:32:13.846281Z","shell.execute_reply":"2022-04-10T12:32:13.846308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r predicted_masks_post.zip ./predicted_masks_post/*\n!rm -r ./predicted_masks_post\n!rm -r ./predicted_masks","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.84775Z","iopub.status.idle":"2022-04-10T12:32:13.848406Z","shell.execute_reply.started":"2022-04-10T12:32:13.848169Z","shell.execute_reply":"2022-04-10T12:32:13.848196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(18, 18))\n# img = cv2.imread('./predicted_masks/7.jpg', cv2.COLOR_RGB2GRAY)\n# img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n# # Get rid of JPG artifacts\n# img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)[1]\n\n# # Create structuring elements\n# horizontal_size = 50\n# vertical_size = 50\n# horizontalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (horizontal_size, 5))\n# verticalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (5, vertical_size))\n\n# # Morphological opening\n# mask1 = cv2.morphologyEx(img, cv2.MORPH_OPEN, horizontalStructure)\n# mask2 = cv2.morphologyEx(img, cv2.MORPH_OPEN, verticalStructure)\n\n# plt.imshow(mask1)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.849649Z","iopub.status.idle":"2022-04-10T12:32:13.850317Z","shell.execute_reply.started":"2022-04-10T12:32:13.850054Z","shell.execute_reply":"2022-04-10T12:32:13.850097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(18, 18))\n# img = cv2.imread('./predicted_masks/7.jpg')\n# imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n# ret, thresh = cv2.threshold(imgray, 127, 255, 0)\n# contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n# c = cv2.drawContours(img, contours, -1, (0,255,0), thickness=cv2.FILLED)\n\n# plt.imshow(c)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.851648Z","iopub.status.idle":"2022-04-10T12:32:13.852338Z","shell.execute_reply.started":"2022-04-10T12:32:13.852074Z","shell.execute_reply":"2022-04-10T12:32:13.852117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(18, 18))\n# img = cv2.imread('./predicted_masks/7.jpg')\n# img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n# img = cv2.medianBlur(img, 21)\n# plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.853746Z","iopub.status.idle":"2022-04-10T12:32:13.854492Z","shell.execute_reply.started":"2022-04-10T12:32:13.854221Z","shell.execute_reply":"2022-04-10T12:32:13.854249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(18, 18))\n# img = cv2.imread('./predicted_masks/7.jpg')\n# # plt.imshow(img)\n# # gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n# # blur = cv2.GaussianBlur(gray, (7,7), 0)\n# # ret, BW = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY)\n# # plt.imshow(BW)\n# gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n# thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n# kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,10))\n# close = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=1)\n\n# plt.imshow(close)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.859715Z","iopub.status.idle":"2022-04-10T12:32:13.860396Z","shell.execute_reply.started":"2022-04-10T12:32:13.860157Z","shell.execute_reply":"2022-04-10T12:32:13.860184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## plt.figure(figsize=(18, 18))\n# bw = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \\\n#                                 cv2.THRESH_BINARY, 15, -2)\n# # plt.imshow(bw)\n# horizontal = np.copy(bw)\n# vertical = np.copy(bw)\n\n# # Specify size on horizontal axis\n# cols = horizontal.shape[1]\n# horizontal_size = cols // 30\n# # Create structure element for extracting horizontal lines through morphology operations\n# horizontalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (horizontal_size, 1))\n# # Apply morphology operations\n# horizontal = cv2.erode(horizontal, horizontalStructure)\n# horizontal = cv2.dilate(horizontal, horizontalStructure)\n\n# # Specify size on vertical axis\n# rows = vertical.shape[0]\n# verticalsize = rows // 30\n# # Create structure element for extracting vertical lines through morphology operations\n# verticalStructure = cv2.getStructuringElement(cv2.MORPH_RECT, (1, verticalsize))\n# # Apply morphology operations\n# vertical = cv2.erode(vertical, verticalStructure)\n# vertical = cv2.dilate(vertical, verticalStructure)\n\n# # Inverse vertical image\n# vertical = cv2.bitwise_not(vertical)\n# '''\n# Extract edges and smooth image according to the logic\n# 1. extract edges\n# 2. dilate(edges)\n# 3. src.copyTo(smooth)\n# 4. blur smooth img\n# 5. smooth.copyTo(src, edges)\n# '''\n# # Step 1\n# edges = cv2.adaptiveThreshold(vertical, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \\\n#                             cv2.THRESH_BINARY, 3, -2)\n# # Step 2\n# kernel = np.ones((2, 2), np.uint8)\n# edges = cv2.dilate(edges, kernel)\n# # Step 3\n# smooth = np.copy(vertical)\n# # Step 4\n# smooth = cv2.blur(smooth, (2, 2))\n# # Step 5\n# (rows, cols) = np.where(edges != 0)\n# vertical[rows, cols] = smooth[rows, cols]\n# # Show final result\n# # plt.imshow(vertical)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:32:13.861673Z","iopub.status.idle":"2022-04-10T12:32:13.86233Z","shell.execute_reply.started":"2022-04-10T12:32:13.862098Z","shell.execute_reply":"2022-04-10T12:32:13.862126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(18, 18))\n# kernel = np.ones((5,5),np.uint8)\n# erosion = cv2.erode(img,kernel,iterations = 1)\n# dilation = cv2.dilate(img,kernel,iterations = 1)\n# opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n# closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n# gradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)\n# plt.imshow(gradient)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}